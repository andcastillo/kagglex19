{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.naive_bayes import GaussianNB\n#  importando los modulos confusion_matrix, roc_curve, auc, accuracy_score de sklearn.metrics\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score, classification_report\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/datatn-bancolombia-2019/DT19_Datos_transaccionales_predict.csv\n/kaggle/input/datatn-bancolombia-2019/DT19_Datos_transaccionales_train.csv\n/kaggle/input/datatn-bancolombia-2019/Metadatos Datos_Var_Rpta.xlsx\n/kaggle/input/datatn-bancolombia-2019/DT19_IDs_predict.csv\n/kaggle/input/datatn-bancolombia-2019/Metadatos Datos_Trasaccionales.xlsx\n/kaggle/input/datatn-bancolombia-2019/Trminos y Condiciones Datatn 2019.pdf\n/kaggle/input/datatn-bancolombia-2019/presentacin Dataton BC 2019.pdf\n/kaggle/input/datatn-bancolombia-2019/DT19_maestro_cdgtrn_cdgrpta.csv\n/kaggle/input/datatn-bancolombia-2019/DT19_Datos_Var_Rpta_train_lite.csv\n/kaggle/input/datatn-bancolombia-2019/DT19_Datos_transaccionales_train_lite.csv\n/kaggle/input/datatn-bancolombia-2019/Metadatos maestro_cdgtrn_cdgrpta.xlsx\n/kaggle/input/datatn-bancolombia-2019/DT19_Datos_Var_Rpta_train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataY = pd.read_csv('/kaggle/input/datatn-bancolombia-2019/DT19_Datos_Var_Rpta_train.csv')\ndataX = pd.read_csv('/kaggle/input/datatn-bancolombia-2019/DT19_Datos_transaccionales_train.csv')\n","execution_count":141,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataY.head()","execution_count":142,"outputs":[{"output_type":"execute_result","execution_count":142,"data":{"text/plain":"   id  f_analisis  var_rpta  segmento\n0   1      201803         0         4\n1   2      201604         0         0\n2   3      201608         0         5\n3   4      201706         0         4\n4   5      201703         0         4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f_analisis</th>\n      <th>var_rpta</th>\n      <th>segmento</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>201803</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>201604</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>201608</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>201706</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>201703</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataX.shape","execution_count":143,"outputs":[{"output_type":"execute_result","execution_count":143,"data":{"text/plain":"(40533683, 8)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataX.loc[dataX['id'] == 1, ['id', 'sesion']].tail(6)","execution_count":144,"outputs":[{"output_type":"execute_result","execution_count":144,"data":{"text/plain":"        id  sesion\n102632   1       2\n102633   1       2\n102634   1       2\n102635   1       2\n102636   1       1\n102637   1       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sesion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>102632</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>102633</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>102634</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>102635</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>102636</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>102637</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Define function to Group by client id the amount of transactions on each chanel and merge the different parts of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFeatures(dataX): \n    grp = dataX.groupby(['id', 'canal'])['sesion'].nunique()\n    trxcanal = grp.unstack(level=-1)\n    trxcanal.columns = trxcanal.columns.values\n    trxcanal = trxcanal.fillna(0)\n    trxcanal['id'] = trxcanal.index.values\n    del trxcanal.index.name\n    return trxcanal\n\ndef mergeParts(dataA, dataB):\n    data = pd.merge(dataA, dataB, left_on = 'id', right_on = 'id')\n    #Convert segmento from categorical to dummy\n    dummies = pd.get_dummies(data['segmento'], prefix = 'segmento')\n    data = pd.concat([data.drop('segmento', axis = 1), dummies], axis = 1)\n    return data","execution_count":168,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Join the train info with the transactional info"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = mergeParts(dataY, getFeatures(dataX))\ndata.head()","execution_count":169,"outputs":[{"output_type":"execute_result","execution_count":169,"data":{"text/plain":"   id  f_analisis  var_rpta  CANAL_1  CANAL_2  CANAL_3  CANAL_4  CANAL_5  \\\n0   1      201803         0      0.0      0.0      0.0      0.0     69.0   \n1   2      201604         0      0.0      0.0      0.0      0.0      2.0   \n2   3      201608         0      0.0      3.0      0.0      0.0      0.0   \n3   4      201706         0      0.0      0.0      0.0      0.0    618.0   \n4   5      201703         0      0.0      1.0      0.0      0.0     70.0   \n\n   CANAL_6  CANAL_7  CANAL_8  segmento_0  segmento_1  segmento_2  segmento_3  \\\n0      0.0      0.0      0.0           0           0           0           0   \n1      0.0      0.0      0.0           1           0           0           0   \n2      0.0      0.0      0.0           0           0           0           0   \n3      1.0      0.0      0.0           0           0           0           0   \n4     46.0      0.0      2.0           0           0           0           0   \n\n   segmento_4  segmento_5  \n0           1           0  \n1           0           0  \n2           0           1  \n3           1           0  \n4           1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f_analisis</th>\n      <th>var_rpta</th>\n      <th>CANAL_1</th>\n      <th>CANAL_2</th>\n      <th>CANAL_3</th>\n      <th>CANAL_4</th>\n      <th>CANAL_5</th>\n      <th>CANAL_6</th>\n      <th>CANAL_7</th>\n      <th>CANAL_8</th>\n      <th>segmento_0</th>\n      <th>segmento_1</th>\n      <th>segmento_2</th>\n      <th>segmento_3</th>\n      <th>segmento_4</th>\n      <th>segmento_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>201803</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>69.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>201604</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>201608</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>201706</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>618.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>201703</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>70.0</td>\n      <td>46.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## lets try a very simple model"},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames = data.columns.values.tolist()\npredictors = colnames[3:17]\ntargetName = 'var_rpta'","execution_count":147,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Very simple experimental setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new array with the added features: features_two\n\nfeatures = data[predictors]\ntarget = data[targetName]\n# Split the data into train and test\ntrainX, testX, trainY, testY = train_test_split(features, target, test_size = 0.2, stratify = target )\n\nprint('Training: ')\nprint(trainX.shape, trainY.shape)\nprint(trainY.sum()/trainY.count())\nprint('Test: ')\nprint(testX.shape, testY.shape)\nprint(testY.sum()/testY.count())\n","execution_count":148,"outputs":[{"output_type":"stream","text":"Training: \n(61228, 14) (61228,)\n0.07218919448618279\nTest: \n(15308, 14) (15308,)\n0.07218447870394565\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## We need to undersample the dataset to avoid the unbalance of classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler()\nX_res, y_res = rus.fit_resample(trainX, trainY)","execution_count":149,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# NN is sensitive to data scale. We must normilize\nscaler = StandardScaler()  \ntrainXX = X_res\ntestXX = testX.copy()\n# Don't cheat - fit only on training data\nscaler.fit(trainX)  \ntrainXX = scaler.transform(trainXX)  \n# apply same transformation to test data\ntestXX = scaler.transform(testXX)  \n\nmodel3 = MLPClassifier(solver='adam', shuffle=True, learning_rate_init= 0.002, batch_size=200, alpha=1e-2, activation = 'tanh', max_iter=5000, tol = 1e-8, hidden_layer_sizes = [10, 8, 6, 4], learning_rate='adaptive', verbose = True)\nmodel3.fit(trainXX, y_res)                     \n\n#Print the score on the train data\nprint(\"On training\")\nprint(model3.score(trainXX, y_res))\nprint(confusion_matrix(model3.predict(trainXX), y_res))\n\n#Print the score on the test data\nprint(\"\\nOn test\")\nprint(model3.score(testXX, testY))\nprint(confusion_matrix(model3.predict(testXX), testY))\n\nprint(\"ROC: \")\nprint(roc_auc_score(testY, model3.predict(testXX)))\nprint(roc_auc_score(y_res, model3.predict(trainXX)))","execution_count":167,"outputs":[{"output_type":"stream","text":"Iteration 1, loss = 0.67864785\nIteration 2, loss = 0.64597875\nIteration 3, loss = 0.63385497\nIteration 4, loss = 0.63033339\nIteration 5, loss = 0.62678466\nIteration 6, loss = 0.62478363\nIteration 7, loss = 0.62382570\nIteration 8, loss = 0.62180395\nIteration 9, loss = 0.62063614\nIteration 10, loss = 0.61916873\nIteration 11, loss = 0.61933071\nIteration 12, loss = 0.61859746\nIteration 13, loss = 0.61777566\nIteration 14, loss = 0.61761245\nIteration 15, loss = 0.61688656\nIteration 16, loss = 0.61647033\nIteration 17, loss = 0.61604005\nIteration 18, loss = 0.61592082\nIteration 19, loss = 0.61549770\nIteration 20, loss = 0.61514128\nIteration 21, loss = 0.61514695\nIteration 22, loss = 0.61426536\nIteration 23, loss = 0.61521323\nIteration 24, loss = 0.61423210\nIteration 25, loss = 0.61423656\nIteration 26, loss = 0.61387887\nIteration 27, loss = 0.61419949\nIteration 28, loss = 0.61278195\nIteration 29, loss = 0.61401823\nIteration 30, loss = 0.61286828\nIteration 31, loss = 0.61244450\nIteration 32, loss = 0.61233087\nIteration 33, loss = 0.61235661\nIteration 34, loss = 0.61238381\nIteration 35, loss = 0.61218777\nIteration 36, loss = 0.61278309\nIteration 37, loss = 0.61159089\nIteration 38, loss = 0.61126332\nIteration 39, loss = 0.61176642\nIteration 40, loss = 0.61143797\nIteration 41, loss = 0.61220907\nIteration 42, loss = 0.61116278\nIteration 43, loss = 0.61167259\nIteration 44, loss = 0.61081174\nIteration 45, loss = 0.61097854\nIteration 46, loss = 0.61083364\nIteration 47, loss = 0.61076297\nIteration 48, loss = 0.61016085\nIteration 49, loss = 0.61044691\nIteration 50, loss = 0.61052606\nIteration 51, loss = 0.61068750\nIteration 52, loss = 0.61004601\nIteration 53, loss = 0.60993963\nIteration 54, loss = 0.60943622\nIteration 55, loss = 0.61026257\nIteration 56, loss = 0.60927676\nIteration 57, loss = 0.61016024\nIteration 58, loss = 0.60928743\nIteration 59, loss = 0.60962099\nIteration 60, loss = 0.60944176\nIteration 61, loss = 0.60896061\nIteration 62, loss = 0.60913544\nIteration 63, loss = 0.60866423\nIteration 64, loss = 0.60866194\nIteration 65, loss = 0.60877065\nIteration 66, loss = 0.60886098\nIteration 67, loss = 0.60875198\nIteration 68, loss = 0.60864532\nIteration 69, loss = 0.60882975\nIteration 70, loss = 0.60859954\nIteration 71, loss = 0.60884723\nIteration 72, loss = 0.60756760\nIteration 73, loss = 0.60958969\nIteration 74, loss = 0.60829037\nIteration 75, loss = 0.60897246\nIteration 76, loss = 0.60819006\nIteration 77, loss = 0.60860995\nIteration 78, loss = 0.60904952\nIteration 79, loss = 0.60807968\nIteration 80, loss = 0.60780505\nIteration 81, loss = 0.60815591\nIteration 82, loss = 0.60811320\nIteration 83, loss = 0.60853710\nTraining loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\nOn training\n0.6559954751131222\n[[2988 1609]\n [1432 2811]]\n\nOn test\n0.6607656127515025\n[[9441  431]\n [4762  674]]\nROC: \n0.6373367362640547\n0.6559954751131222\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainXX","execution_count":86,"outputs":[{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"array([[ 0.29047319, -0.14282447, -0.18676381, ..., -0.50331468,\n        -0.29827208, -0.29827208],\n       [ 0.29047319, -0.14282447, -0.24366063, ...,  0.28727991,\n         1.88549259,  1.88549259],\n       [-1.77258039, -0.14282447,  0.21151393, ..., -0.32419559,\n         0.22583144,  0.22583144],\n       ...,\n       [-1.77258039, -0.14282447, -0.01607335, ..., -0.42919644,\n        -0.29827208, -0.29827208],\n       [-1.25681699, -0.14282447, -0.35745427, ..., -0.50331468,\n        -0.29827208, -0.29827208],\n       [ 0.80623659, -0.14282447,  0.09772029, ..., -0.50331468,\n        -0.29827208, -0.29827208]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Make a new prediction and save it in the output file"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2Y = pd.read_csv('/kaggle/input/datatn-bancolombia-2019/DT19_IDs_predict.csv')\ndata2X = pd.read_csv('/kaggle/input/datatn-bancolombia-2019/DT19_Datos_transaccionales_predict.csv')\n\ndata2 = mergeParts(data2Y, getFeatures(data2X))\nfeatures = data2[predictors]\n\ntest2XX = scaler.transform(features)  \n#Make a prediction\ndata2Y['probabilidad'] =  model3.predict(test2XX)\n\ndata_final = data2Y[['id', 'probabilidad']]\n\n# Save to a file. Find it in /kaggle/output/working/\ndata_final.to_csv(\"DT19_IDs_predict.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":172,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
